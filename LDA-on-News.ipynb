{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN749+54Rh3qoRGZBXmHafu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FLQvdudb6Uqd"},"outputs":[],"source":["import pandas as pd\n","\n","data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n","data_text = data[['headline_text']]\n","data_text['index'] = data_text.index\n","documents = data_text"]},{"cell_type":"code","source":["len(documents)"],"metadata":{"id":"AlTmBieT7BO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documents[:5]"],"metadata":{"id":"UkxH3qwn7B2K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Pre-processing"],"metadata":{"id":"XOoTbwsg7PZl"}},{"cell_type":"code","source":["import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import numpy as np\n","np.random.seed(2018)"],"metadata":{"id":"4s30Oh1H7FS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"id":"EYScb6NY7VZ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lemmatize Example"],"metadata":{"id":"wCpBjRC-7fr2"}},{"cell_type":"code","source":["print(WordNetLemmatizer().lemmatize('went', pos='v'))"],"metadata":{"id":"7kp98CYI7d47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stemmer Example"],"metadata":{"id":"Hoyp18Pt7p4W"}},{"cell_type":"code","source":["stemmer = SnowballStemmer('english')\n","original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n","           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n","           'traditional', 'reference', 'colonizer','plotted']\n","singles = [stemmer.stem(plural) for plural in original_words]\n","pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"],"metadata":{"id":"sK6zmHTD7nET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result"],"metadata":{"id":"olSIkbq97wUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc_sample = documents[documents['index'] == 4310].values[0][0]\n","\n","print('original document: ')\n","words = []\n","for word in doc_sample.split(' '):\n","    words.append(word)\n","print(words)\n","print('\\n\\n tokenized and lemmatized document: ')\n","print(preprocess(doc_sample))"],"metadata":{"id":"YL06jQnU7ySC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_docs = documents['headline_text'].map(preprocess)"],"metadata":{"id":"ly7RKS7o73cx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_docs[:10]"],"metadata":{"id":"nqeMoL6575Rt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bag of Words On Dataset"],"metadata":{"id":"VBE-TxBW78-u"}},{"cell_type":"code","source":["dictionary = gensim.corpora.Dictionary(processed_docs)"],"metadata":{"id":"rHW8GUwQ77fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","for k, v in dictionary.iteritems():\n","    print(k, v)\n","    count += 1\n","    if count > 10:\n","        break"],"metadata":{"id":"ZExn3_ij8HDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"],"metadata":{"id":"qUXON7mD8J5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","bow_corpus[4310]"],"metadata":{"id":"QgV805Sr8UEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bow_doc_4310 = bow_corpus[4310]\n","\n","for i in range(len(bow_doc_4310)):\n","    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n","                                                     dictionary[bow_doc_4310[i][0]], \n","                                                     bow_doc_4310[i][1]))"],"metadata":{"id":"PEYGwNHT8V3i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TF-IDF"],"metadata":{"id":"A3b2j00z9Th1"}},{"cell_type":"code","source":["from gensim import corpora, models\n","\n","tfidf = models.TfidfModel(bow_corpus)"],"metadata":{"id":"h-hbri0n8Xu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_tfidf = tfidf[bow_corpus]"],"metadata":{"id":"dwlIErv09Y3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pprint import pprint\n","\n","for doc in corpus_tfidf:\n","    pprint(doc)\n","    break"],"metadata":{"id":"DQtCidiV9aj6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Running LDA using Bag of Words"],"metadata":{"id":"SLWJqE-W9fVW"}},{"cell_type":"code","source":["lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"],"metadata":{"id":"pS5uyfu39dTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, topic in lda_model.print_topics(-1):\n","    print('Topic: {} \\nWords: {}'.format(idx, topic))"],"metadata":{"id":"1_pjwgRy9mtb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Running LDA using TF-IDF "],"metadata":{"id":"vTaF4e2v9qfX"}},{"cell_type":"code","source":["lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"],"metadata":{"id":"LslWU8-t9o0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, topic in lda_model_tfidf.print_topics(-1):\n","    print('Topic: {} Word: {}'.format(idx, topic))"],"metadata":{"id":"s7-3Hq1f9xkC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Classification of Topics"],"metadata":{"id":"xVP2E-BT96nG"}},{"cell_type":"markdown","source":["## Performance evaluation by classifying sample document using LDA Bag of Words model"],"metadata":{"id":"tNdnlGJt9-G-"}},{"cell_type":"code","source":["processed_docs[4310]"],"metadata":{"id":"IyOI0ykq91JS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n","    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"],"metadata":{"id":"-t-V2hVB-Et6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Performance evaluation by classifying sample document using LDA TF-IDF model"],"metadata":{"id":"hPBZFj4j-Jb9"}},{"cell_type":"code","source":["for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n","    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"],"metadata":{"id":"0Kyqj6eT-GhK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing model on unseen document"],"metadata":{"id":"vY8Xzzl5-RAt"}},{"cell_type":"code","source":["unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n","bow_vector = dictionary.doc2bow(preprocess(unseen_document))"],"metadata":{"id":"dNObJorE-OsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n","    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"],"metadata":{"id":"kz5NRpF1-U4f"},"execution_count":null,"outputs":[]}]}