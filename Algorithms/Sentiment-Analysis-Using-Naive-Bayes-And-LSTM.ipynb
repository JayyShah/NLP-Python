{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOCbXIbqjYuv4yF5O01UdZd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# loading the Dataset"],"metadata":{"id":"Ac3JiR2nPNV6"}},{"cell_type":"code","source":["import re\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split \n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","import math\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer\n","import collections\n","from collections import defaultdict\n","from google.colab import drive \n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVmzPfLtXGNn","executionInfo":{"status":"ok","timestamp":1673351878826,"user_tz":-330,"elapsed":5008,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}},"outputId":"e502eeab-76d4-44ae-ccfe-10aa3bc48b30"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/NLP-Python/data/IMDB Dataset.csv')\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"jmc0PrDqXz7g","executionInfo":{"status":"ok","timestamp":1673351880085,"user_tz":-330,"elapsed":1263,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}},"outputId":"244e4a4c-d563-441e-9fed-7feff0372ee0"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"],"text/html":["\n","  <div id=\"df-3086a683-2b2a-43e1-a221-e0c5c0d6f45c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3086a683-2b2a-43e1-a221-e0c5c0d6f45c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3086a683-2b2a-43e1-a221-e0c5c0d6f45c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3086a683-2b2a-43e1-a221-e0c5c0d6f45c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# Data Pre-Processing "],"metadata":{"id":"22d_mf7CZQiX"}},{"cell_type":"code","source":["def remove_tags(strings):\n","  removelist = \"\"\n","  result = re.sub('','',strings) # Remove HTML tags\n","  result = re.sub('https://.*','',result) # Remove URLS\n","  result = re.sub(r'[^w'+removelist+']','',result) # Remove non-alphanumeric charecters\n","  result = result.lower()\n","  return result\n","\n","data['review'] = data['review'].apply(lambda cw : remove_tags(cw))\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","data['review'] = data['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ti61BMaGZU6b","executionInfo":{"status":"ok","timestamp":1673351897448,"user_tz":-330,"elapsed":17366,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}},"outputId":"4bab2009-6a7b-4c9b-b55b-b6286ff6fe58"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# Lemmatization"],"metadata":{"id":"enHjdSMYdVE6"}},{"cell_type":"markdown","source":["lemmatization is used to find the root form of words or lemmas in NLP. This helps save unnecessary computational overhead in trying to decipher entire words."],"metadata":{"id":"BSbdBrqGewwZ"}},{"cell_type":"code","source":["# !pip install nltk --upgrade\n","w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","def lemmatize_text(text):\n","    st = \"\"\n","    for w in w_tokenizer.tokenize(text):\n","        st = st + lemmatizer.lemmatize(w) + \" \"\n","    return st\n","data['review'] = data.review.apply(lemmatize_text)"],"metadata":{"id":"B3j7OySbekxt","executionInfo":{"status":"ok","timestamp":1673352027012,"user_tz":-330,"elapsed":1096,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Encoding labels and Making Train-test Splits"],"metadata":{"id":"ygX0jexCgsJr"}},{"cell_type":"code","source":["reviews = data['review'].values\n","labels = data['sentiment'].values\n","encoder = LabelEncoder()\n","encoded_labels = encoder.fit_transform(labels)"],"metadata":{"id":"BCgWug9VjF0d","executionInfo":{"status":"ok","timestamp":1673352124823,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["The Data is then split into 80-20 splits from sklearn.model_selection"],"metadata":{"id":"VPinkDcpj5_4"}},{"cell_type":"code","source":["train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify = encoded_labels)"],"metadata":{"id":"s3ZH3lCrjxGJ","executionInfo":{"status":"ok","timestamp":1673352287754,"user_tz":-330,"elapsed":473,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Building the Naive Bayes Classifier "],"metadata":{"id":"SLV5ujmXkYxB"}},{"cell_type":"code","source":["vec = CountVectorizer(max_features = 3000)\n","X = vec.fit_transform(train_sentences)\n","vocab = vec.get_feature_names()\n","X = X.toarray()\n","word_counts = {}\n","for l in range(2):\n","    word_counts[l] = defaultdict(lambda: 0)\n","for i in range(X.shape[0]):\n","    l = train_labels[i]\n","    for j in range(len(vocab)):\n","        word_counts[l][vocab[j]] += X[i][j]"],"metadata":{"id":"LWVSDd1hkqGP","executionInfo":{"status":"ok","timestamp":1673352770088,"user_tz":-330,"elapsed":4721,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Laplace smoothing - Takes the vocabulary and the raw word_counts dictionary and returns the smoothened conditional probabilities"],"metadata":{"id":"FR-njNOImg_B"}},{"cell_type":"code","source":["def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n","  a = word_counts[text_label][word] + 1\n","  b = n_label_items[text_label] + len(vocab)\n","  return math.log(a/b)"],"metadata":{"id":"4eHO9wL2mZAg","executionInfo":{"status":"ok","timestamp":1673353195013,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def group_by_label(x, y, labels):\n","    data = {}\n","    for l in labels:\n","        data[l] = x[np.where(y == l)]\n","    return data"],"metadata":{"id":"VL10W7EuvCCC","executionInfo":{"status":"ok","timestamp":1673355083819,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def fit(x, y, labels):\n","    n_label_items = {}\n","    log_label_priors = {}\n","    n = len(x)\n","    grouped_data = group_by_label(x, y, labels)\n","    for l, data in grouped_data.items():\n","        n_label_items[l] = len(data)\n","        log_label_priors[l] = math.log(n_label_items[l] / n)\n","    return n_label_items, log_label_priors"],"metadata":{"id":"jzE-8IKDn2Rq","executionInfo":{"status":"ok","timestamp":1673355083819,"user_tz":-330,"elapsed":2,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n","    result = []\n","    for text in x:\n","        label_scores = {l: log_label_priors[l] for l in labels}\n","        words = set(w_tokenizer.tokenize(text))\n","        for word in words:\n","            if word not in vocab: continue\n","            for l in labels:\n","                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n","                label_scores[l] += log_w_given_l\n","        result.append(max(label_scores, key=label_scores.get))\n","    return result"],"metadata":{"id":"9J-q4La6p6bj","executionInfo":{"status":"ok","timestamp":1673355086504,"user_tz":-330,"elapsed":11,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# Fitting the Model on Training SEt and evaluating accuracies on the test set"],"metadata":{"id":"-nZJ0HiQp_56"}},{"cell_type":"code","source":["labels = [0,1]\n","n_label_items, log_label_priors = fit(train_sentences, train_labels, labels)\n","pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n","print(\"Accuracy of prediction on Test set: \",accuracy_score(test_labels, pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xNzmZJSqJmK","executionInfo":{"status":"ok","timestamp":1673355086505,"user_tz":-330,"elapsed":10,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}},"outputId":"54a6c39b-b5f9-43e8-fa20-c3add13bcb3a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of prediction on Test set:  0.51832\n"]}]},{"cell_type":"markdown","source":["## Key Takeaways About NB Classifier:\n","  1. A Naive bayes classifier is a probabilistic ML classifier based on Bayes Theorem.\n","  2. Laplace Smoothing needs to be performed while calculating feature conditional probabilities so that, unseen words which are not in the trainig corpus can be handled"],"metadata":{"id":"ylis0aOQvW3z"}},{"cell_type":"code","source":[],"metadata":{"id":"hBh7TqNywMHM"},"execution_count":null,"outputs":[]}]}